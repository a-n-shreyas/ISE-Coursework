# ISE-Coursework

ğŸ“ Introduction
Bug report classification is a crucial aspect of software maintenance, helping developers efficiently triage and resolve issues. However, traditional methods like NaÃ¯ve Bayes with TF-IDF often struggle, particularly when dealing with imbalanced datasets, where non-bug reports significantly outnumber actual bug reports.

This project explores machine learning approaches to improve bug report classification by:
âœ” Evaluating multiple datasets (Pytorch, Keras, Caffe, TensorFlow, incubator-mxnet).

âœ” Comparing a NaÃ¯ve Bayes baseline against an improved Random Forest model.

âœ” Using Word2Vec embeddings for feature extraction instead of TF-IDF.

âœ” Applying sentiment analysis (VADER) to analyze comments.

âœ” Employing SMOTE to handle class imbalance and enhance recall.

âœ” Conducting rigorous statistical validation to ensure improvements are significant.

Our results demonstrate that Random Forest outperforms NaÃ¯ve Bayes across all datasets, but challenges remain in achieving production-level performance. This project provides insights into the limitations of traditional classifiers and suggests future improvements using BERT-based embeddings and hybrid models.

ğŸš€ Whether you're a machine learning enthusiast, software engineer, or researcher, this project serves as a foundation for improving bug classification models!

ğŸ“Œ What Youâ€™ll Find in This Repository

ğŸ“‚ Dataset-Based Performance Comparisons (Precision, Recall, F1-Score)

ğŸ“Š Statistical Analysis & Significance Testing

ğŸ›  Reproducible Code for ML Training & Evaluation

ğŸ“œ Detailed Observations & Future Work Recommendations

ğŸ“Š Performance Comparison Across Datasets

ğŸ”¹ Precision

| Dataset          | NaÃ¯ve Bayes | Random Forest |
|-----------------|------------|--------------|
| Pytorch        | 1.000      | 0.313        |
| Keras          | 1.000      | 0.281        |
| incubator-mxnet | 1.000     | 0.318        |
| Caffe          | 1.000      | 0.290        |
| TensorFlow     | 1.000      | 0.309        |

ğŸ”¹ F1-Score
| Dataset          | NaÃ¯ve Bayes | Random Forest |
|-----------------|------------|--------------|
| Pytorch        | 0.000      | 0.130        |
| Keras          | 0.005      | 0.112        |
| incubator-mxnet | 0.000     | 0.115        |
| Caffe          | 0.000      | 0.111        |
| TensorFlow     | 0.019      | 0.116        |

ğŸ”¹ Recall
| Dataset          | NaÃ¯ve Bayes | Random Forest |
|-----------------|------------|--------------|
| Pytorch        | 0.000      | 0.091        |
| Keras          | 0.002      | 0.074        |
| incubator-mxnet | 0.000     | 0.075        |
| Caffe          | 0.000      | 0.075        |
| TensorFlow     | 0.010      | 0.075        |

ğŸ“Œ Key Observations

ğŸš¨ Consistent Baseline Failure

NaÃ¯ve Bayes achieved perfect precision (1.0) across all datasets but zero recall/F1-score except for Keras and TensorFlow, where F1 < 0.02.

Cause: The classifier only predicts the majority class (87-90% non-bug reports), making it ineffective for minority class detection.

ğŸš€ Random Forest Superiority

Statistically significant improvement over NaÃ¯ve Bayes (p < 0.00001, Wilcoxon test).

Best Performance: Pytorch (F1 = 0.130)

Worst Performance: Caffe (F1 = 0.111)

âš– Precision-Recall Tradeoff

Highest Precision: incubator-mxnet (0.318)

Highest Recall: Pytorch (0.091)

ğŸ“ˆ Dataset Variability
| Metric         | Highest Variance (Std Dev) |
|---------------|--------------------------|
| Precision      | TensorFlow (0.282)      |
| F1-score      | Keras (0.081)           |
| Recall        | TensorFlow (0.056)      |

ğŸ“Œ Statistical Validation

ğŸ“‰ Normality Tests

NaÃ¯ve Bayes results: W = 1.0, p = 1.0 (No variance).

Random Forest results: Non-normal distributions (p < 0.05).

ğŸ“Š Effect Sizes (Cliffâ€™s Delta for F1-score)

| Dataset          | Cliffâ€™s Delta (F1) |
|-----------------|-------------------|
| Pytorch        | 0.91 (Large)       |
| Keras          | 0.89 (Large)       |
| incubator-mxnet | 0.90 (Large)      |
| Caffe          | 0.87 (Large)       |
| TensorFlow     | 0.85 (Large)       |

ğŸ“Œ Reflection & Future Improvements

ğŸ”´ Limitations

Low Absolute Performance:

Despite improvements, the highest F1-score is only 0.130 (Pytorch), far below production thresholds (F1 > 0.7).

Root Cause: Limited contextual understanding using Word2Vec instead of BERT-based embeddings.

Cross-Dataset Generalization:

Worst performance on Caffe (F1 = 0.111) despite similar conditions.

Possible Reason: Domain-specific jargon not being captured effectively.

ğŸ’¡ Improvement Pathways

âœ” Feature Engineering Enhancements:

Introduce domain-specific lexicons (e.g., CUDA error for Pytorch/TensorFlow).

Use FastText instead of Word2Vec to improve handling of rare words.


âœ” Threshold Optimization:

Adjust decision thresholds per dataset (e.g., 0.25 for Pytorch vs. 0.15 for Caffe).


âœ” Model Architecture Enhancements:

Replace Word2Vec with DistilBERT for better contextual feature extraction.

Explore hybrid ML + Deep Learning models for better performance.


ğŸ“Œ Conclusion

âœ… Random Forest significantly outperforms NaÃ¯ve Bayes (p < 0.00001) across all datasets.

âŒ Absolute performance remains inadequate for real-world deployment.

ğŸ’¡ Key takeaways:

Statistical evaluation beyond accuracy is critical.

Domain-specific feature engineering impacts generalization.

Traditional ML models struggle with semantic-heavy tasks.


ğŸ”® Future Work:

Integrate BERT-based embeddings into classification models.

Explore hybrid transformer + ensemble methods for better performance.

ğŸ“Œ Repository Artifacts
ğŸ“‚ GitHub Repository: [Insert Link]

âœ… Whatâ€™s Included?

ğŸ“Š Results for 5 datasets

ğŸ–¥ Reproducible code for all experiments

ğŸ“‰ Statistical analysis scripts

ğŸ“Œ How to Use This Repository?
Clone the repository

git clone [(https://github.com/a-n-shreyas/ISE-Coursework.git)![image](https://github.com/user-attachments/assets/c1832d33-6ecf-4b3e-8148-0f068c0abaa7))]


Install dependencies

pip install -r requirements.txt


python model_training.py

Analyze the results

python statistical_analysis.py

ğŸ“Œ Contributors
ğŸ‘¨â€ğŸ’» Annavati Shreyas â€“ Lead Developer & Researcher

ğŸ“Œ License
ğŸ”– This project is licensed under the MIT License â€“ see the LICENSE file for details.

ğŸ“Œ References
ğŸ“š Key Papers & Studies

[1] T. Hall et al., "A Systematic Literature Review on Fault Classification," IEEE TSE, 2018.

[2] A. Lamkanfi et al., "Predicting Severity of Bug Reports," MSR, 2010.

[3] J. Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers," NAACL, 2019.

[4] C. Tantithamthavorn et al., "Automated Software Quality Assessment," EMSE, 2018.
